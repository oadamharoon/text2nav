{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35b53198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from transformers import ViTFeatureExtractor, ViTImageProcessor\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ffabf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INPUT_PKL = Path(\"/Users/samtett/Documents/Python/Project/replay_buffer_random_orientation.pkl\")\n",
    "CLIP_OUTPUT = INPUT_PKL.parent / \"replay_buffer_with_clip_embeddings.pkl\"\n",
    "VIT_OUTPUT = INPUT_PKL.parent / \"replay_buffer_with_vit_embeddings.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec9c0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPPatcher:\n",
    "    def __init__(self, ckpt=\"openai/clip-vit-base-patch32\", device=None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.proc   = CLIPProcessor.from_pretrained(ckpt)\n",
    "        self.model  = CLIPModel.from_pretrained(ckpt).to(self.device).eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def _l2(x):\n",
    "        return x / x.norm(dim=-1, keepdim=True).clamp(min=1e-6)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_joint_embeddings(self, image: Image.Image, prompts: list[str]) -> np.ndarray:\n",
    "        if not prompts:\n",
    "            h = self.model.config.projection_dim\n",
    "            return np.empty((0, h), dtype=np.float32)\n",
    "\n",
    "        img_inp = self.proc(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        img_feat = self._l2(self.model.get_image_features(**img_inp))\n",
    "        txt_inp = self.proc(text=prompts, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        txt_feat = self._l2(self.model.get_text_features(**txt_inp))\n",
    "        joint = self._l2(img_feat + txt_feat)\n",
    "        return joint.cpu().numpy().copy()\n",
    "\n",
    "def compute_goal_embeddings(rgb_array, tasks_dict, goal_index, color_index, embedding_size):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ddbb69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    matcher = CLIPMatcher()\n",
    "    matcher = CLIPMatcher()\n",
    "    print(\"🚀 Loading replay buffer...\")\n",
    "    with INPUT_PKL.open(\"rb\") as f:\n",
    "        buffer = pickle.load(f)\n",
    "\n",
    "    # test_limit = 1\n",
    "    print(f\"Buffer size: {len(buffer.buffer)}\")\n",
    "    \n",
    "    observations = np.array([experience[0] for experience in buffer.buffer])#[:test_limi\n",
    "    actions = np.array([experience[1] for experience in buffer.buffer])#[:test_limit]\n",
    "    rewards = np.array([experience[2] for experience in buffer.buffer])#[:test_limit]\n",
    "    # next_observations = np.array([experience[3] for experience in buffer.buffer])#[:test_limit]\n",
    "    dones = np.array([experience[4] for experience in buffer.buffer])#[:test_limit]\n",
    "\n",
    "    rgb = np.array([obs['rgb'] for obs in observations]).transpose(0, 1, 4, 2, 3)  # (episodes, steps, channels, height, width)\n",
    "    # next_rgb = np.array([obs['rgb'] for obs in next_observations]).transpose(0, 1, 4, 2, 3)  # (episodes, steps, channels, height, width)\n",
    "    goal_index = np.array([obs['goal_index'] for obs in observations])  # (episodes, steps, 1)\n",
    "\n",
    "    if rgb is None or goal_index is None:\n",
    "        print(\"❌ Missing required data in buffer: rgb, next_rgb or goal_index.\")\n",
    "        return\n",
    "\n",
    "    num_episodes, num_steps = rgb.shape[0], rgb.shape[1]\n",
    "    print(f\"✅ Loaded buffer with {num_episodes} episodes, {num_steps} steps each.\")\n",
    "\n",
    "    print(\"💬 Computing joint embeddings...\")\n",
    "    for ep in tqdm(range(num_episodes), desc=\"Embedding Episodes\"):\n",
    "        for st in range(num_steps):\n",
    "            task_list = tasks_dict[ep][st]\n",
    "\n",
    "            img_np = rgb[ep, st]\n",
    "            if isinstance(img_np, torch.Tensor):\n",
    "                img_np = img_np.cpu().numpy()\n",
    "            img_np = img_np.transpose(1, 2, 0).astype(np.uint8)\n",
    "            if img_np.shape[-1] == 4:\n",
    "                img_np = img_np[..., :3]\n",
    "\n",
    "            img_pil = Image.fromarray(img_np)\n",
    "\n",
    "            if not task_list:\n",
    "                # No object in frame → Add \"move around\" task\n",
    "                prompts = [\"No balls in frame, move around\"]\n",
    "                embs = matcher.get_joint_embeddings(img_pil, prompts)\n",
    "                tasks_dict[ep][st] = [{\n",
    "                    \"task\": prompts[0],\n",
    "                    \"location\": None,\n",
    "                    \"colour\": None,\n",
    "                    \"bbox\": None,\n",
    "                    \"embedding\": embs[0]\n",
    "                }]\n",
    "            else:\n",
    "                prompts = [t[\"task\"] for t in task_list]\n",
    "                embs = matcher.get_joint_embeddings(img_pil, prompts)\n",
    "                for t, e in zip(task_list, embs):\n",
    "                    t[\"embedding\"] = e\n",
    "\n",
    "            if not task_list:\n",
    "                # No object in frame → Add \"move around\" task\n",
    "                prompts = [\"No balls in frame, move around\"]\n",
    "                embs = matcher.get_joint_embeddings(img_pil, prompts)\n",
    "                tasks_dict[ep][st] = [{\n",
    "                    \"task\": prompts[0],\n",
    "                    \"location\": None,\n",
    "                    \"colour\": None,\n",
    "                    \"bbox\": None,\n",
    "                    \"embedding\": embs[0]\n",
    "                }]\n",
    "            else:\n",
    "                prompts = [t[\"task\"] for t in task_list]\n",
    "                embs = matcher.get_joint_embeddings(img_pil, prompts)\n",
    "                for t, e in zip(task_list, embs):\n",
    "                    t[\"embedding\"] = e\n",
    "\n",
    "    # Compute final embeddings based on goal_index\n",
    "    print(\"📦 Computing goal-based embeddings...\")\n",
    "    embedding_size = matcher.model.config.text_config.projection_size\n",
    "    print(f\"Embedding size: {embedding_size}\")\n",
    "    color_index = {\n",
    "        \"red\": 0,\n",
    "        \"green\": 1,\n",
    "        \"blue\": 2,\n",
    "        \"yellow\": 3,\n",
    "        \"magenta\": 4\n",
    "    }\n",
    "\n",
    "    embeddings = compute_goal_embeddings(rgb, tasks_dict, goal_index, color_index, embedding_size)\n",
    "\n",
    "    buffer_with_embeddings = []\n",
    "    buffer_with_embeddings.append(embeddings)\n",
    "    buffer_with_embeddings.append(actions)\n",
    "    buffer_with_embeddings.append(rewards)\n",
    "    buffer_with_embeddings.append(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "930ae319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving final buffer with embeddings...\n",
      "✅ Done. Embeddings saved to: /Users/samtett/Documents/Python/Project/replay_buffer_with_clip_embeddings.pkl\n",
      "✅ Done. Embeddings saved to: /Users/samtett/Documents/Python/Project/replay_buffer_with_clip_embeddings.pkl\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CLIPMatcher' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Done. Embeddings saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCLIP_OUTPUT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     matcher = \u001b[43mCLIPMatcher\u001b[49m()\n\u001b[32m      3\u001b[39m     matcher = CLIPMatcher()\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🚀 Loading replay buffer...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'CLIPMatcher' is not defined"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "print(\"💾 Saving final buffer with embeddings...\")\n",
    "with CLIP_OUTPUT.open(\"wb\") as f:\n",
    "        pickle.dump(CLIP_OUTPUT, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickle.dump(CLIP_OUTPUT, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"✅ Done. Embeddings saved to: {CLIP_OUTPUT}\")\n",
    "print(f\"✅ Done. Embeddings saved to: {CLIP_OUTPUT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b5ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a single object (e.g., a list of embeddings)\n",
    "with open('clip_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(CLIP_OUTPUT, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d0d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data\n",
    "with open(INPUT_PKL, \"rb\") as f:\n",
    "    buffer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b58b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load CLIP\n",
    "clip_model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_id)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_id)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = clip_model.to(device)\n",
    "\n",
    "# Image Embedding\n",
    "def get_clip_image_embedding(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        img_emb = clip_model.get_image_features(**inputs)\n",
    "    return img_emb.cpu().numpy().squeeze()\n",
    "\n",
    "# Text Embedding\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        text_emb = clip_model.get_text_features(**inputs)\n",
    "    return text_emb.cpu().numpy().squeeze()\n",
    "\n",
    "def compute_goal_embeddings(rgb_array, tasks_dict, goal_index, color_index, embedding_size):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28c392c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/samtett/Documents/Python/.venv/lib/python3.13/site-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "\n",
    "# Load ViT\n",
    "vit_model_id = \"google/vit-base-patch16-224\"\n",
    "vit_model = ViTModel.from_pretrained(vit_model_id)\n",
    "vit_processor = ViTFeatureExtractor.from_pretrained(vit_model_id)\n",
    "vit_model = vit_model.to(device)\n",
    "\n",
    "# Image Embedding\n",
    "def get_vit_image_embedding(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = vit_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = vit_model(**inputs)\n",
    "        # Use the [CLS] token embedding as the image representation\n",
    "        img_emb = outputs.last_hidden_state[:, 0, :]\n",
    "    return img_emb.cpu().numpy().squeeze()\n",
    "\n",
    "def compute_goal_embeddings(rgb_array, tasks_dict, goal_index, color_index, embedding_size):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769aa7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78034235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a single object (e.g., a list of embeddings)\n",
    "with open('clip_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(CLIP_OUTPUT, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('vit_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(VIT_OUTPUT, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

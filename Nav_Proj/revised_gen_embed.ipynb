{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed"
      ],
      "metadata": {
        "id": "o1w9oNfZhzO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_PKL = Path(\"/work/mech-ai-scratch/nitesh/workspace/text2nav/buffer_with_orientations.pkl\")\n",
        "\n",
        "# Choose model type: 'siglip', 'clip', or 'vilt'\n",
        "MODEL_TYPE = \"vilt\"\n",
        "OUTPUT_PKL = INPUT_PKL.parent / f\"replay_buffer_with_embeddings_{MODEL_TYPE}.pkl\""
      ],
      "metadata": {
        "id": "JNFQdrGehzkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SiglipProcessor, SiglipModel\n",
        "\n",
        "class SigLIPMatcher:\n",
        "    def __init__(self, model_name=\"google/siglip-so400m-patch14-384\", device=None):\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.processor = SiglipProcessor.from_pretrained(model_name)\n",
        "        self.model = SiglipModel.from_pretrained(model_name).to(self.device).eval()\n",
        "\n",
        "    def _l2(self, x):\n",
        "        return x / x.norm(dim=-1, keepdim=True).clamp(min=1e-6)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_joint_embeddings(self, images, prompts):\n",
        "        img_inp = self.processor(images=images, return_tensors=\"pt\").to(self.device)\n",
        "        img_feat = self._l2(self.model.get_image_features(**img_inp))\n",
        "        txt_inp = self.processor(text=prompts, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "        txt_feat = self._l2(self.model.get_text_features(**txt_inp))\n",
        "        joint = self._l2(img_feat + txt_feat)\n",
        "        return joint"
      ],
      "metadata": {
        "id": "w7db5_Jmh3eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "class CLIPMatcher:\n",
        "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\", device=None):\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "        self.model = CLIPModel.from_pretrained(model_name).to(self.device).eval()\n",
        "\n",
        "    def _l2(self, x):\n",
        "        return x / x.norm(dim=-1, keepdim=True).clamp(min=1e-6)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_joint_embeddings(self, images, prompts):\n",
        "        # Process images and text separately for CLIP\n",
        "        img_inputs = self.processor(images=images, return_tensors=\"pt\").to(self.device)\n",
        "        txt_inputs = self.processor(text=prompts, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "\n",
        "        # Get image and text features separately\n",
        "        img_feat = self._l2(self.model.get_image_features(**img_inputs))\n",
        "        txt_feat = self._l2(self.model.get_text_features(**txt_inputs))\n",
        "\n",
        "        # Combine features (similar to SigLIP approach)\n",
        "        joint = self._l2(img_feat + txt_feat)\n",
        "        return joint"
      ],
      "metadata": {
        "id": "qsD9Oye8h4_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViltProcessor, ViltModel\n",
        "\n",
        "class ViLTMatcher:\n",
        "    def __init__(self, model_name=\"dandelin/vilt-b32-finetuned-vqa\", device=None):\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.processor = ViltProcessor.from_pretrained(model_name)\n",
        "        self.model = ViltModel.from_pretrained(model_name).to(self.device).eval()\n",
        "\n",
        "    def _l2(self, x):\n",
        "        return x / x.norm(dim=-1, keepdim=True).clamp(min=1e-6)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_joint_embeddings(self, images, prompts):\n",
        "        # ViLT processes image and text together\n",
        "        inputs = self.processor(images=images, text=prompts, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "        outputs = self.model(**inputs)\n",
        "\n",
        "        # Use pooler_output for joint representation\n",
        "        pooled_output = outputs.pooler_output  # shape: (batch_size, hidden_size)\n",
        "        return self._l2(pooled_output)"
      ],
      "metadata": {
        "id": "1vlBwAs5h6cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingPipeline:\n",
        "    def __init__(self, model_type=\"siglip\"):\n",
        "        \"\"\"\n",
        "        Initialize embedding pipeline with specified model type.\n",
        "\n",
        "        Args:\n",
        "            model_type (str): One of 'siglip', 'clip', or 'vilt'\n",
        "        \"\"\"\n",
        "        self.model_type = model_type\n",
        "        if model_type == \"siglip\":\n",
        "            self.matcher = SigLIPMatcher()\n",
        "        elif model_type == \"clip\":\n",
        "            self.matcher = CLIPMatcher()\n",
        "        elif model_type == \"vilt\":\n",
        "            self.matcher = ViLTMatcher()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}. Choose from 'siglip', 'clip', or 'vilt'\")\n",
        "\n",
        "        print(f\"Initialized {model_type.upper()} embedding pipeline\")\n",
        "\n",
        "    def generate(self, image: torch.Tensor, prompts):\n",
        "        \"\"\"\n",
        "        Generate joint embeddings for the given image and prompts.\n",
        "\n",
        "        Args:\n",
        "            image (torch.Tensor): Input image tensor. (N, 3, 256, 256)\n",
        "            prompts (list): List of text prompts\n",
        "\n",
        "        Returns:\n",
        "            embeddings (torch.Tensor): Tensor of joint embeddings.\n",
        "        \"\"\"\n",
        "        embeddings = self.matcher.get_joint_embeddings(image, prompts)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "yh9zitBliGaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the replay buffer\n",
        "with open(INPUT_PKL, \"rb\") as f:\n",
        "    buffer = pickle.load(f)\n",
        "\n",
        "print(f\"Loaded buffer with {len(buffer)} items\")"
      ],
      "metadata": {
        "id": "kZe4CW-GiIoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components from replay buffer\n",
        "replay_buffer = buffer\n",
        "rgbs = np.array([item[0] for item in replay_buffer])\n",
        "goal_indices = np.array([item[1] for item in replay_buffer])\n",
        "angles = np.array([item[2] for item in replay_buffer])\n",
        "actions = np.array([item[3] for item in replay_buffer])\n",
        "rewards = np.array([item[4] for item in replay_buffer])\n",
        "dones = np.array([item[5] for item in replay_buffer])\n",
        "truncateds = np.array([item[6] for item in replay_buffer])\n",
        "\n",
        "print(f\"Data shapes: RGB={rgbs.shape}, Goals={goal_indices.shape}, Angles={angles.shape}\")\n",
        "print(f\"Actions={actions.shape}, Rewards={rewards.shape}, Dones={dones.shape}, Truncated={truncateds.shape}\")"
      ],
      "metadata": {
        "id": "ugTdqVH9iKLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Color mapping for prompts\n",
        "COLOR_INDEX = {\n",
        "    \"red\": 0,\n",
        "    \"green\": 1,\n",
        "    \"blue\": 2,\n",
        "    \"yellow\": 3,\n",
        "    \"pink\": 4\n",
        "}\n",
        "INDEX_COLOR = {v: k for k, v in COLOR_INDEX.items()}\n",
        "\n",
        "print(f\"Color mapping: {INDEX_COLOR}\")"
      ],
      "metadata": {
        "id": "jbKqaro0iLxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_relative_prompts(y, goal_index, INDEX_COLOR, y_thresh=0.2, base=\"Move toward the ball.\"):\n",
        "    \"\"\"\n",
        "    Generate relative position prompts based on goal angle.\n",
        "\n",
        "    Args:\n",
        "        y: angle values (relative y-coordinate)\n",
        "        goal_index: target color indices\n",
        "        INDEX_COLOR: mapping from index to color name\n",
        "        y_thresh: threshold for left/right classification\n",
        "        base: base instruction text\n",
        "\n",
        "    Returns:\n",
        "        List of prompt strings\n",
        "    \"\"\"\n",
        "    prompts = []\n",
        "\n",
        "    for i in range(len(y)):\n",
        "        color = INDEX_COLOR[int(goal_index[i])]\n",
        "        if y[i] > y_thresh:\n",
        "            prompts.append(f\"The target is {color} ball which is to your left. {base}\")\n",
        "        elif y[i] < -y_thresh:\n",
        "            prompts.append(f\"The target is {color} ball which is to your right. {base}\")\n",
        "        else:\n",
        "            prompts.append(f\"The target is {color} ball which is straight ahead. {base}\")\n",
        "\n",
        "    return prompts"
      ],
      "metadata": {
        "id": "-LHiI_isiNNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize pipeline with the MODEL_TYPE set at the top\n",
        "embedding_pipeline = EmbeddingPipeline(model_type=MODEL_TYPE)"
      ],
      "metadata": {
        "id": "UJvOJx0tiO5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def thread_worker(i):\n",
        "    \"\"\"\n",
        "    Worker function for parallel processing of embeddings.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        rgb = rgbs[i]\n",
        "        angle = angles[i]\n",
        "        goal_index = goal_indices[i]\n",
        "        prompts = generate_relative_prompts(angle, goal_index, INDEX_COLOR=INDEX_COLOR)\n",
        "        embedding = embedding_pipeline.generate(rgb, prompts)\n",
        "        return i, embedding.cpu().numpy()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing item {i}: {e}\")\n",
        "        return i, None"
      ],
      "metadata": {
        "id": "XEREl43qi2O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings using parallel processing\n",
        "embeddings_buffer = [None] * len(rgbs)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    futures = [executor.submit(thread_worker, i) for i in range(len(rgbs))]\n",
        "\n",
        "    for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Generating {MODEL_TYPE.upper()} embeddings\"):\n",
        "        idx, embedding = future.result()\n",
        "        if embedding is not None:\n",
        "            embeddings_buffer[idx] = embedding\n",
        "        else:\n",
        "            print(f\"Failed to process item {idx}\")\n",
        "\n",
        "# Convert to numpy array\n",
        "embeddings_buffer = np.array(embeddings_buffer)\n",
        "print(f\"Generated embeddings shape: {embeddings_buffer.shape}\")"
      ],
      "metadata": {
        "id": "qQTvefCni29y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create replay buffer with embeddings\n",
        "replay_buffer_with_embeddings = []\n",
        "for i in range(len(rgbs)):\n",
        "    replay_buffer_with_embeddings.append((\n",
        "        embeddings_buffer[i],\n",
        "        actions[i],\n",
        "        rewards[i],\n",
        "        dones[i],\n",
        "        truncateds[i]\n",
        "    ))\n",
        "\n",
        "print(f\"Created replay buffer with {len(replay_buffer_with_embeddings)} items\")"
      ],
      "metadata": {
        "id": "WezvuLGwi6NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save embeddings buffer for future use\n",
        "embeddings_filename = f\"embeddings_buffer_{MODEL_TYPE}.pkl\"\n",
        "with open(embeddings_filename, \"wb\") as f:\n",
        "    pickle.dump(replay_buffer_with_embeddings, f)\n",
        "\n",
        "print(f\"Embeddings saved as {embeddings_filename}\")"
      ],
      "metadata": {
        "id": "9faTYxFIkG6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and slice data for episode boundaries\n",
        "embedds = np.array([item[0] for item in replay_buffer_with_embeddings])\n",
        "actions_rb = np.array([item[1] for item in replay_buffer_with_embeddings])\n",
        "rewards_rb = np.array([item[2] for item in replay_buffer_with_embeddings])\n",
        "dones_rb = np.array([item[3] for item in replay_buffer_with_embeddings])\n",
        "truncateds_rb = np.array([item[4] for item in replay_buffer_with_embeddings])\n",
        "\n",
        "print(f\"Data shapes before slicing:\")\n",
        "print(f\"Embeddings: {embedds.shape}, Actions: {actions_rb.shape}\")\n",
        "print(f\"Rewards: {rewards_rb.shape}, Dones: {dones_rb.shape}, Truncated: {truncateds_rb.shape}\")"
      ],
      "metadata": {
        "id": "QnMu6o7jjXRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slice data based on episode boundaries\n",
        "embedds_sliced = []\n",
        "actions_sliced = []\n",
        "rewards_sliced = []\n",
        "dones_sliced = []\n",
        "truncateds_sliced = []\n",
        "\n",
        "for i in range(embedds.shape[1]):\n",
        "    done_mask = dones_rb[:, 0, :] > 0\n",
        "    truncated_mask = truncateds_rb[:, 0, :] > 0\n",
        "\n",
        "    # Element-wise OR to find episode ends\n",
        "    final_mask = np.logical_or(done_mask, truncated_mask)\n",
        "\n",
        "    # Get the indices where episodes end\n",
        "    indices = np.where(final_mask)[0]\n",
        "    if len(indices) > 0:\n",
        "        last_index = indices[-1] + 1\n",
        "    else:\n",
        "        last_index = embedds.shape[0]\n",
        "\n",
        "    embedds_sliced.append(embedds[:last_index, i, :])\n",
        "    actions_sliced.append(actions_rb[:last_index, i, :])\n",
        "    rewards_sliced.append(rewards_rb[:last_index, i, :])\n",
        "    dones_sliced.append(dones_rb[:last_index, i, :])\n",
        "    truncateds_sliced.append(truncateds_rb[:last_index, i, :])\n",
        "\n",
        "# Stack all sliced data\n",
        "embedds_sliced = np.vstack(embedds_sliced)\n",
        "actions_sliced = np.vstack(actions_sliced)\n",
        "rewards_sliced = np.vstack(rewards_sliced)\n",
        "dones_sliced = np.vstack(dones_sliced)\n",
        "truncateds_sliced = np.vstack(truncateds_sliced)\n",
        "\n",
        "print(f\"Data shapes after slicing:\")\n",
        "print(f\"Embeddings: {embedds_sliced.shape}, Actions: {actions_sliced.shape}\")\n",
        "print(f\"Rewards: {rewards_sliced.shape}, Dones: {dones_sliced.shape}, Truncated: {truncateds_sliced.shape}\")"
      ],
      "metadata": {
        "id": "t2mk0L13jbzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import d3rlpy\n",
        "from d3rlpy.dataset import MDPDataset\n",
        "from d3rlpy.algos import TD3PlusBCConfig\n",
        "from d3rlpy.preprocessing import MinMaxActionScaler"
      ],
      "metadata": {
        "id": "DHUXxH0zli6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(embeddings, actions, rewards, dones, truncateds):\n",
        "    \"\"\"\n",
        "    Create MDP dataset for d3rlpy training.\n",
        "    \"\"\"\n",
        "    dataset = MDPDataset(\n",
        "        observations=embeddings,\n",
        "        actions=actions,\n",
        "        rewards=rewards,\n",
        "        terminals=dones,\n",
        "        timeouts=truncateds,\n",
        "        action_space=d3rlpy.constants.ActionSpace.CONTINUOUS,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "# Create dataset\n",
        "dataset = create_dataset(\n",
        "    embeddings=embedds_sliced,\n",
        "    actions=actions_sliced,\n",
        "    rewards=rewards_sliced,\n",
        "    dones=dones_sliced,\n",
        "    truncateds=truncateds_sliced\n",
        ")\n",
        "\n",
        "print(f\"Created dataset with {len(dataset)} transitions\")"
      ],
      "metadata": {
        "id": "-0WdQEmejvt0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
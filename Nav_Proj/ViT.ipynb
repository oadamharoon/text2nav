{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samtett/Documents/Python/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from transformers import ViTFeatureExtractor, ViTImageProcessor\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51d2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INPUT_PKL = Path(\"/Users/samtett/Documents/Python/Project/replay_buffer_random_orientation.pkl\")\n",
    "VIT_OUTPUT = INPUT_PKL.parent / \"replay_buffer_with_vit_embeddings.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc66573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTMatcher:\n",
    "    def __init__(self, ckpt=\"openai/clip-vit-base-patch32\", device=None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.proc   = ViTImageProcessor.from_pretrained(ckpt)\n",
    "        self.model  = ViTModel.from_pretrained(ckpt).to(self.device).eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def _l2(x):\n",
    "        return x / x.norm(dim=-1, keepdim=True).clamp(min=1e-6)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_joint_embeddings(self, image: Image.Image, prompts: list[str]) -> np.ndarray:\n",
    "        if not prompts:\n",
    "            h = self.model.config.projection_dim\n",
    "            return np.empty((0, h), dtype=np.float32)\n",
    "\n",
    "        img_inp = self.proc(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        img_feat = self._l2(self.model.get_image_features(**img_inp))\n",
    "        txt_inp = self.proc(text=prompts, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        txt_feat = self._l2(self.model.get_text_features(**txt_inp))\n",
    "        joint = self._l2(img_feat + txt_feat)\n",
    "        return joint.cpu().numpy().copy()\n",
    "\n",
    "def compute_goal_embeddings(rgb_array, tasks_dict, goal_index, color_index, embedding_size):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ab6211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    matcher = ViTMatcher()\n",
    "    matcher = ViTMatcher()\n",
    "    print(\"üöÄ Loading replay buffer...\")\n",
    "    with INPUT_PKL.open(\"rb\") as f:\n",
    "        buffer = pickle.load(f)\n",
    "\n",
    "    # test_limit = 1\n",
    "    print(f\"Buffer size: {len(buffer.buffer)}\")\n",
    "    \n",
    "    observations = np.array([experience[0] for experience in buffer.buffer])#[:test_limi\n",
    "    actions = np.array([experience[1] for experience in buffer.buffer])#[:test_limit]\n",
    "    rewards = np.array([experience[2] for experience in buffer.buffer])#[:test_limit]\n",
    "    # next_observations = np.array([experience[3] for experience in buffer.buffer])#[:test_limit]\n",
    "    dones = np.array([experience[4] for experience in buffer.buffer])#[:test_limit]\n",
    "\n",
    "    rgb = np.array([obs['rgb'] for obs in observations]).transpose(0, 1, 4, 2, 3)  # (episodes, steps, channels, height, width)\n",
    "    # next_rgb = np.array([obs['rgb'] for obs in next_observations]).transpose(0, 1, 4, 2, 3)  # (episodes, steps, channels, height, width)\n",
    "    goal_index = np.array([obs['goal_index'] for obs in observations])  # (episodes, steps, 1)\n",
    "\n",
    "    if rgb is None or goal_index is None:\n",
    "        print(\"‚ùå Missing required data in buffer: rgb, next_rgb or goal_index.\")\n",
    "        return\n",
    "\n",
    "    num_episodes, num_steps = rgb.shape[0], rgb.shape[1]\n",
    "    print(f\"‚úÖ Loaded buffer with {num_episodes} episodes, {num_steps} steps each.\")\n",
    "\n",
    "    print(\"üí¨ Computing joint embeddings...\")\n",
    "    for ep in tqdm(range(num_episodes), desc=\"Embedding Episodes\"):\n",
    "        for st in range(num_steps):\n",
    "            task_list = tasks_dict[ep][st]\n",
    "\n",
    "            img_np = rgb[ep, st]\n",
    "            if isinstance(img_np, torch.Tensor):\n",
    "                img_np = img_np.cpu().numpy()\n",
    "            img_np = img_np.transpose(1, 2, 0).astype(np.uint8)\n",
    "            if img_np.shape[-1] == 4:\n",
    "                img_np = img_np[..., :3]\n",
    "\n",
    "            img_pil = Image.fromarray(img_np)\n",
    "\n",
    "            if not task_list:\n",
    "                # No object in frame ‚Üí Add \"move around\" task\n",
    "                prompts = [\"No balls in frame, move around\"]\n",
    "                embs = matcher.get_joint_embeddings(img_pil, prompts)\n",
    "                tasks_dict[ep][st] = [{\n",
    "                    \"task\": prompts[0],\n",
    "                    \"location\": None,\n",
    "                    \"colour\": None,\n",
    "                    \"bbox\": None,\n",
    "                    \"embedding\": embs[0]\n",
    "                }]\n",
    "            else:\n",
    "                prompts = [t[\"task\"] for t in task_list]\n",
    "                embs = matcher.get_joint_embeddings(img_pil, prompts)\n",
    "                for t, e in zip(task_list, embs):\n",
    "                    t[\"embedding\"] = e\n",
    "\n",
    "            if not task_list:\n",
    "                # No object in frame ‚Üí Add \"move around\" task\n",
    "                prompts = [\"No balls in frame, move around\"]\n",
    "                embs = matcher.get_joint_embeddings(img_pil, prompts)\n",
    "                tasks_dict[ep][st] = [{\n",
    "                    \"task\": prompts[0],\n",
    "                    \"location\": None,\n",
    "                    \"colour\": None,\n",
    "                    \"bbox\": None,\n",
    "                    \"embedding\": embs[0]\n",
    "                }]\n",
    "            else:\n",
    "                prompts = [t[\"task\"] for t in task_list]\n",
    "                embs = matcher.get_joint_embeddings(img_pil, prompts)\n",
    "                for t, e in zip(task_list, embs):\n",
    "                    t[\"embedding\"] = e\n",
    "\n",
    "    # Compute final embeddings based on goal_index\n",
    "    print(\"üì¶ Computing goal-based embeddings...\")\n",
    "    embedding_size = matcher.model.config.text_config.projection_size\n",
    "    print(f\"Embedding size: {embedding_size}\")\n",
    "    color_index = {\n",
    "        \"red\": 0,\n",
    "        \"green\": 1,\n",
    "        \"blue\": 2,\n",
    "        \"yellow\": 3,\n",
    "        \"magenta\": 4\n",
    "    }\n",
    "\n",
    "    embeddings = compute_goal_embeddings(rgb, tasks_dict, goal_index, color_index, embedding_size)\n",
    "\n",
    "    buffer_with_embeddings = []\n",
    "    buffer_with_embeddings.append(embeddings)\n",
    "    buffer_with_embeddings.append(actions)\n",
    "    buffer_with_embeddings.append(rewards)\n",
    "    buffer_with_embeddings.append(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc3c5732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving final buffer with embeddings...\n",
      "‚úÖ Done. Embeddings saved to: /Users/samtett/Documents/Python/Project/replay_buffer_with_vit_embeddings.pkl\n",
      "‚úÖ Done. Embeddings saved to: /Users/samtett/Documents/Python/Project/replay_buffer_with_vit_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"üíæ Saving final buffer with embeddings...\")\n",
    "with open(\"vit_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(VIT_OUTPUT, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(VIT_OUTPUT, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(f\"‚úÖ Done. Embeddings saved to: {VIT_OUTPUT}\")\n",
    "    print(f\"‚úÖ Done. Embeddings saved to: {VIT_OUTPUT}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
